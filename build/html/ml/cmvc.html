

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Crossmodal Voice Conversion &mdash; 音声信号処理のための備忘録  documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/graphviz.css?v=4ae1632d" />

  
      <script src="../_static/documentation_options.js?v=5929fcd5"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
      <script>window.MathJax = {"loader": {"load": ["[tex]/bussproofs"]}, "tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "packages": {"[+]": ["bussproofs"]}}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            音声信号処理のための備忘録
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../pachouli/index.html">書きたいものまとめ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../paper/index.html">論文まとめ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../implementation/index.html">実装</a></li>
<li class="toctree-l1"><a class="reference internal" href="../logic/index.html">論理学</a></li>
<li class="toctree-l1"><a class="reference internal" href="../calculus/index.html">微分積分学</a></li>
<li class="toctree-l1"><a class="reference internal" href="../processing/index.html">信号処理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../speech_analysis/index.html">音響解析</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html">machine learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../control_engineering/index.html">制御工学</a></li>
<li class="toctree-l1"><a class="reference internal" href="../a.html">集合と位相</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">音声信号処理のための備忘録</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Crossmodal Voice Conversion</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/ml/cmvc.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="crossmodal-voice-conversion">
<h1>Crossmodal Voice Conversion<a class="headerlink" href="#crossmodal-voice-conversion" title="Link to this heading">¶</a></h1>
<section id="id1">
<h2>数式的説明<a class="headerlink" href="#id1" title="Link to this heading">¶</a></h2>
<p><span class="math notranslate nohighlight">\(\mathbf{x}=[\mathbf{x}_1,...,\mathbf{x}_N]\in \mathbb{R}^{D\times N}\)</span> を発話音声の音響特徴量のベクトル列とし、 <span class="math notranslate nohighlight">\(\mathbf{y} \in \mathbb{R}^{I\times J}\)</span> を話者に対応する顔画像とする。</p>
<p>ここで、 同時確率分布 <span class="math notranslate nohighlight">\(p(\mathbf{x}, \mathbf{y})\)</span> を２つのVAEによってモデル化する。</p>
<p>音声の符号化器(Utterance Encoder)は <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> を時間依存な潜在変数列 <span class="math notranslate nohighlight">\(\mathbf{z}=[\mathbf{z}_1,...,\mathbf{z}_{N'}] \in \mathbb{R}^{D' \times N'}\)</span> 符号化することを目的とする。
復号化器(Utterrance Decoder) は潜在変数 <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> と補助特徴量から <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> を再合成することを目的とする。
<span class="math notranslate nohighlight">\(\mathbf{z}\)</span> が言語情報のみを捕らえ、 <span class="math notranslate nohighlight">\(\mathbf{c}\)</span> から話者性に関する情報を得ることを理想としている。
したがって、符号化器と復号化器が音声認識や音声合成の音響モデルとして機能し、補助入力 <span class="math notranslate nohighlight">\(\mathbf{c}\)</span> に応じて入力発話音声を変換できるようになることが期待される。</p>
<p>また、補助特徴量 <span class="math notranslate nohighlight">\(\mathbf{c}\)</span> は、顔画像の符号化器(Face Encoder)によって得られる時間非依存な潜在変数を用いる。
<span class="math notranslate nohighlight">\(\mathbf{c}\)</span> についても復号化器(Face Decoder) によって画像 <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> の再合成を行う。</p>
<p>(画像1にp(x,y)のグラフを書いているが俺も書く?)</p>
<p>この生成モデルは以下のような条件付き確率分布で表現される。 Utterance/Face Decoder及びUtterance/Face Encoderはそれぞれ <span class="math notranslate nohighlight">\(p_{\theta_\mathrm{a}}(\mathbf{x}|\mathbf{z},\mathbf{c}), p_{\theta_\mathrm{v}}(\mathbf{y}|\mathbf{c}),q_{\phi_\mathrm{a}}(\mathbf{z}|\mathbf{x}), q_{\phi_\mathrm{v}}(\mathbf{c}|\mathbf{y})\)</span> であり、 <span class="math notranslate nohighlight">\(\theta_\mathrm{a},\theta_\mathrm{v},\phi_\mathrm{a},\phi_\mathrm{v}\)</span> はニューラルネットワークのパラメータである。</p>
<p>このモデルは実際の事後確率 <span class="math notranslate nohighlight">\(p(\mathbf{z},\mathbf{c}|\mathbf{x},\mathbf{y}) \propto p_{\theta_\mathrm{a}}(\mathbf{x}|\mathbf{z},\mathbf{c})p_{\theta_\mathrm{v}}(\mathbf{y}|\mathbf{c})\)</span> を <span class="math notranslate nohighlight">\(q(\mathbf{z},\mathbf{c}|\mathbf{x},\mathbf{y}) = q_{\phi_\mathrm{a}}(\mathbf{z}|\mathbf{x})q_{\phi_\mathrm{v}}(\mathbf{c}|\mathbf{y})\)</span> によって近似する。
この時、これらの分布の間におけるKLダイバージェンスは、</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathrm{KL}[q(\mathbf{z},&amp;\mathbf{c}|\mathbf{x},\mathbf{y})||p(\mathbf{z},\mathbf{c}|\mathbf{x},\mathbf{y})] = \log{p(x,y)} \\
&amp;- \mathbb{E}_{q_{\phi_\mathrm{v}}(\mathbf{c}|\mathbf{y}),q_{\phi_\mathrm{a}}(\mathbf{z}|\mathbf{x})}[\log{p_{\theta_\mathrm{a}}(\mathbf{x}|\mathbf{z},\mathbf{c})}] \\
&amp;- \mathbb{E}_{q_{\phi_\mathrm{v}}(\mathbf{c}|\mathbf{y})}[\log{p_{\theta_\mathrm{v}}(\mathbf{y}|\mathbf{c})}] \\
&amp;+ \mathrm{KL}[q_{\phi_\mathrm{a}}(\mathbf{z}|\mathbf{x})||p(\mathbf{z})] + \mathrm{KL}[q_{\phi_\mathrm{v}}(\mathbf{c}|\mathbf{y})||p(\mathbf{c})] \\\end{split}\]</div>
<p>よって、発話と顔画像の訓練ペアデータ <span class="math notranslate nohighlight">\(\{\mathbf{x}_m,\mathbf{y}_m\}^M_{m=1}\)</span> を用いて訓練基準(訳これでいい?)</p>
<div class="math notranslate nohighlight">
\[\begin{split}&amp;\mathcal{J}(\theta_\mathrm{a},\phi_\mathrm{a},\theta_\mathrm{v},\phi_\mathrm{v})\\
&amp;= \mathbb{E}_{p_\mathrm{d}(\mathbf{x},\mathbf{y})}\mathbb{E}_{q_{\phi_\mathrm{v}}(\mathbf{c}|\mathbf{y}),q_{\phi_\mathrm{a}}(\mathbf{z}|\mathbf{x})}[\log{p_{\theta_\mathrm{a}}(\mathbf{x}|\mathbf{z},\mathbf{c})}] \\
&amp;+ \mathbb{E}_{p_\mathrm{d}(\mathbf{y})}\mathbb{E}_{q_{\phi_\mathrm{v}}(\mathbf{c}|\mathbf{y})}[\log{p_{\theta_\mathrm{v}}(\mathbf{y}|\mathbf{c})}] \\
&amp;- \mathbb{E}_{p_\mathrm{d}(\mathbf{x})}\mathrm{KL}[q_{\phi_\mathrm{a}}(\mathbf{z}|\mathbf{x})||p(\mathbf{z})] \\
&amp;- \mathbb{E}_{p_\mathrm{d}(\mathbf{y})}\mathrm{KL}[q_{\phi_\mathrm{v}}(\mathbf{c}|\mathbf{y})||p(\mathbf{c})] \\\end{split}\]</div>
<p>によってパラメータ <span class="math notranslate nohighlight">\(\theta_\mathrm{a},\phi_\mathrm{a},\theta_\mathrm{v},\phi_\mathrm{v}\)</span> の最適化を行う。</p>
<p>(reparametrization trickについて書く?)</p>
<p>Utterance Decoderが補助入力 <span class="math notranslate nohighlight">\(\mathbf{c}\)</span> を利用する方法に明示的な制限がないため、Utterance Decoder出力が <span class="math notranslate nohighlight">\(\mathbf{c}\)</span> に相関するように、情報理論的な正則化項を導入する。
<span class="math notranslate nohighlight">\(\mathbf{z}\)</span> を条件とした、 <span class="math notranslate nohighlight">\(\mathbf{x} \sim p_{\theta_\mathrm{a}}()\)</span> と <span class="math notranslate nohighlight">\(\mathbf{c}\)</span> 相互情報量は以下のように書ける。</p>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, licrum.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>