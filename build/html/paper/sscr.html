

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>One-shot Voice Conversion by Separating Speaker and Content Representations with Instance Normalization &mdash; 音声信号処理のための備忘録  documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/graphviz.css?v=4ae1632d" />

  
      <script src="../_static/documentation_options.js?v=5929fcd5"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Training Robust Zero-Shot Voice Conversion Models with Self-supervised Features" href="rozs.html" />
    <link rel="prev" title="AUTOVC: Zero-Shot Voice Style Transfer with Only Autoencoder Loss" href="autovc.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            音声信号処理のための備忘録
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../pachouli/index.html">書きたいものまとめ</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">論文まとめ</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="an_overview.html">An Overview of Voice Conversion and its Challenges: From Statistical Modeling to Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="vqmivc.html">VQMIVC: Vector Quantization and Mutual Information-Based Unsupervised Speech Representation Disentanglement for One-shot Voice Conversion</a></li>
<li class="toctree-l2"><a class="reference internal" href="autovc.html">AUTOVC: Zero-Shot Voice Style Transfer with Only Autoencoder Loss</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">One-shot Voice Conversion by Separating Speaker and Content Representations with Instance Normalization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#words">words</a></li>
<li class="toctree-l3"><a class="reference internal" href="#abstract">Abstract</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="rozs.html">Training Robust Zero-Shot Voice Conversion Models with Self-supervised Features</a></li>
<li class="toctree-l2"><a class="reference internal" href="vqvae.html">Neural Discrete Representation Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="gakkai/index.html">音響学会</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../implementation/index.html">実装</a></li>
<li class="toctree-l1"><a class="reference internal" href="../logic/index.html">論理学</a></li>
<li class="toctree-l1"><a class="reference internal" href="../calculus/index.html">微分積分学</a></li>
<li class="toctree-l1"><a class="reference internal" href="../processing/index.html">信号処理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../speech_analysis/index.html">音響解析</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ml/index.html">machine learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../control_engineering/index.html">制御工学</a></li>
<li class="toctree-l1"><a class="reference internal" href="../a.html">集合と位相</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">音声信号処理のための備忘録</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="index.html">論文まとめ</a></li>
      <li class="breadcrumb-item active">One-shot Voice Conversion by Separating Speaker and Content Representations with Instance Normalization</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/paper/sscr.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="one-shot-voice-conversion-by-separating-speaker-and-content-representations-with-instance-normalization">
<h1><a class="toc-backref" href="#id2" role="doc-backlink">One-shot Voice Conversion by Separating Speaker and Content Representations with Instance Normalization</a><a class="headerlink" href="#one-shot-voice-conversion-by-separating-speaker-and-content-representations-with-instance-normalization" title="Link to this heading">¶</a></h1>
<p><a class="reference external" href="https://arxiv.org/abs/1904.05742">https://arxiv.org/abs/1904.05742</a></p>
<nav class="contents" id="id1">
<p class="topic-title">目次</p>
<ul class="simple">
<li><p><a class="reference internal" href="#one-shot-voice-conversion-by-separating-speaker-and-content-representations-with-instance-normalization" id="id2">One-shot Voice Conversion by Separating Speaker and Content Representations with Instance Normalization</a></p>
<ul>
<li><p><a class="reference internal" href="#words" id="id3">words</a></p></li>
<li><p><a class="reference internal" href="#abstract" id="id4">Abstract</a></p></li>
</ul>
</li>
</ul>
</nav>
<section id="words">
<h2><a class="toc-backref" href="#id3" role="doc-backlink">words</a><a class="headerlink" href="#words" title="Link to this heading">¶</a></h2>
<p>what is “supervision” ?</p>
<p>Instance Normalization(IN):</p>
</section>
<section id="abstract">
<h2><a class="toc-backref" href="#id4" role="doc-backlink">Abstract</a><a class="headerlink" href="#abstract" title="Link to this heading">¶</a></h2>
<p>Recently, voice conversion (VC) without parallel data has been successfully adapted to multi-target scenario in which a single model is trained to convert the input voice to many different speakers.
However, such model suffers from the limitation that it can only convert the voice to the speakers in the training data, which narrows down the applicable scenario of VC.</p>
<p><strong>In this paper</strong>, we proposed a novel one-shot VC approach which is able to perform VC by only an example utterance from source and target speaker respectively, and the source and target speaker do not even need to be seen during training. This is achieved by disentangling speaker and content representations with instance normalization (IN).</p>
<p>Objective and subjective evaluation shows that our model is able to generate the voice similar to target speaker.
In addition to the performance measurement, we also demonstrate that this model is able to learn meaningful speaker representations without any supervision.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="autovc.html" class="btn btn-neutral float-left" title="AUTOVC: Zero-Shot Voice Style Transfer with Only Autoencoder Loss" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="rozs.html" class="btn btn-neutral float-right" title="Training Robust Zero-Shot Voice Conversion Models with Self-supervised Features" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, licrum.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>