

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Training Robust Zero-Shot Voice Conversion Models with Self-supervised Features &mdash; 音声信号処理のための備忘録  documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/graphviz.css?v=4ae1632d" />

  
      <script src="../_static/documentation_options.js?v=5929fcd5"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Neural Discrete Representation Learning" href="vqvae.html" />
    <link rel="prev" title="One-shot Voice Conversion by Separating Speaker and Content Representations with Instance Normalization" href="sscr.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            音声信号処理のための備忘録
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../pachouli/index.html">書きたいものまとめ</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">論文まとめ</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="an_overview.html">An Overview of Voice Conversion and its Challenges: From Statistical Modeling to Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="vqmivc.html">VQMIVC: Vector Quantization and Mutual Information-Based Unsupervised Speech Representation Disentanglement for One-shot Voice Conversion</a></li>
<li class="toctree-l2"><a class="reference internal" href="autovc.html">AUTOVC: Zero-Shot Voice Style Transfer with Only Autoencoder Loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="sscr.html">One-shot Voice Conversion by Separating Speaker and Content Representations with Instance Normalization</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Training Robust Zero-Shot Voice Conversion Models with Self-supervised Features</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#words">words</a></li>
<li class="toctree-l3"><a class="reference internal" href="#abstract">Abstract</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="vqvae.html">Neural Discrete Representation Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="gakkai/index.html">音響学会</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../implementation/index.html">実装</a></li>
<li class="toctree-l1"><a class="reference internal" href="../logic/index.html">論理学</a></li>
<li class="toctree-l1"><a class="reference internal" href="../calculus/index.html">微分積分学</a></li>
<li class="toctree-l1"><a class="reference internal" href="../processing/index.html">信号処理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../speech_analysis/index.html">音響解析</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ml/index.html">machine learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../control_engineering/index.html">制御工学</a></li>
<li class="toctree-l1"><a class="reference internal" href="../a.html">集合と位相</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">音声信号処理のための備忘録</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="index.html">論文まとめ</a></li>
      <li class="breadcrumb-item active">Training Robust Zero-Shot Voice Conversion Models with Self-supervised Features</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/paper/rozs.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="training-robust-zero-shot-voice-conversion-models-with-self-supervised-features">
<h1><a class="toc-backref" href="#id2" role="doc-backlink">Training Robust Zero-Shot Voice Conversion Models with Self-supervised Features</a><a class="headerlink" href="#training-robust-zero-shot-voice-conversion-models-with-self-supervised-features" title="Link to this heading">¶</a></h1>
<p><a class="reference external" href="https://arxiv.org/abs/2112.04424">https://arxiv.org/abs/2112.04424</a></p>
<nav class="contents" id="id1">
<p class="topic-title">目次</p>
<ul class="simple">
<li><p><a class="reference internal" href="#training-robust-zero-shot-voice-conversion-models-with-self-supervised-features" id="id2">Training Robust Zero-Shot Voice Conversion Models with Self-supervised Features</a></p>
<ul>
<li><p><a class="reference internal" href="#words" id="id3">words</a></p></li>
<li><p><a class="reference internal" href="#abstract" id="id4">Abstract</a></p></li>
</ul>
</li>
</ul>
</nav>
<section id="words">
<h2><a class="toc-backref" href="#id3" role="doc-backlink">words</a><a class="headerlink" href="#words" title="Link to this heading">¶</a></h2>
</section>
<section id="abstract">
<h2><a class="toc-backref" href="#id4" role="doc-backlink">Abstract</a><a class="headerlink" href="#abstract" title="Link to this heading">¶</a></h2>
<p>Unsupervised Zero-Shot Voice Conversion (VC) aims to modify the speaker characteristic of an utterance to match an unseen target speaker without relying on parallel training data.</p>
<p>Recently, self-supervised learning of speech representation has been shown to produce useful linguistic units without using transcripts, which can be directly passed to a VC model.</p>
<p>In this paper, we showed that high-quality audio samples can be achieved by using a length resampling decoder, which enables the VC model to work in conjunction with different linguistic feature extractors and vocoders without requiring them to operate on the same sequence length. We showed that our method can outperform many baselines on the VCTK dataset.</p>
<p><strong>Without modifying the architecture, we further demonstrated that a) using pairs of different audio segments from the same speaker, b) adding a cycle consistency loss and, c) adding a speaker classification loss can help to learn a better speaker embedding.</strong></p>
<p>Our model trained on LibriTTS using these techniques achieves the best performance, producing audio samples transferred well to the target speaker’s voice, while preserving the linguistic content that is comparable with actual human utterances in terms of Character Error Rate.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="sscr.html" class="btn btn-neutral float-left" title="One-shot Voice Conversion by Separating Speaker and Content Representations with Instance Normalization" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="vqvae.html" class="btn btn-neutral float-right" title="Neural Discrete Representation Learning" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, licrum.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>